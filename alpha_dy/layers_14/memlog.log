2024-05-31 01:32:49;INFO;

alpaca Batch Size 1 alpha_dy fine-tuning 14 Layers
2024-05-31 01:32:49;INFO;
Total param      : 6738.423808
Train param      : 291.504128
Dataset          : alpaca
Method           : alpha_dy
Layers           : 14
Batch size       : 1
Learning Rate    : 2e-06
Eval Loss        : 1.0584303140640259
Forward time     : 13.741864124933878 min
Backward time    : 26.00518139998118 min
Weight memory    : 27087.929344 MB
Optimizer memory : 2332.033024 MB
Activation memory: 872.134008370196 MB
Gradient memory  : 1362.5590816928627 MB
Input memory     : 0.0030693898039215687 MB
Total memory     : 30890.786816 MB
Peak memory      : 34187.37664 MB


